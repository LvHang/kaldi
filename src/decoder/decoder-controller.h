// decoder/decoder-controller.h

// Copyright   2018  Hang Lyu

// See ../../COPYING for clarification regarding multiple authors
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//  http://www.apache.org/licenses/LICENSE-2.0
//
// THIS CODE IS PROVIDED *AS IS* BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
// KIND, EITHER EXPRESS OR IMPLIED, INCLUDING WITHOUT LIMITATION ANY IMPLIED
// WARRANTIES OR CONDITIONS OF TITLE, FITNESS FOR A PARTICULAR PURPOSE,
// MERCHANTABLITY OR NON-INFRINGEMENT.
// See the Apache 2 License for the specific language governing permissions and
// limitations under the License.

#ifndef KALDI_DECODER_DECODER_CONTROLLER_H_
#define KALDI_DECODER_DECODER_CONTROLLER_H_

#include "base/timer.h"
#include "util/kaldi-thread.h"
#include "itf/options-itf.h"
#include "decoder/lattice-faster-decoder.h"
#include "decoder/lattice-simple-decoder.h"
#include "nnet3/nnet-am-decodable-simple.h"
#include "decoder/decoder-wrappers.h"

#if HAVE_CUDA == 1
#include "decoder/lattice-faster-decoder-cuda.h"
#endif

namespace kaldi {
namespace nnet3 {
 
/** This class is used in parallel gpu decoding stuff. For this stuff, both
    neural network forwarding and generating lattices are processed in GPU.
    At the same time, the log-likelihood which is generated by neural network
    forwarding(posterior - prior) and received by decoder is stored in GPU
    so that the PCI-E bus will not be a bottleneck of speed.

    For this stuff, we use a "producer-consumer" model. The "producer" is class
    BatchComputerClass which produces the log-likelihood (posterior - prior).
    The "consumer" is class DecodeUtteranceLatticeClassCuda which consume the
    log-likelihood to generate lattice. And the "warehouse" is class Controller
    which is used to keep the log-likelihood and communicate the work of various
    parts.
 */
class Controller {
 public:
  // Initializer sets various variables.
  // The first half is used in the part of batch computing. The second half is
  // used in the part of generating lattices.
  Controller(
    const NnetSimpleComputationOptions &opts,
    const Nnet &nnet,
    const VectorBase<BaseFloat> &priors,
    int32 online_ivector_period,
    bool ensure_exact_final_context,
    int32 minibatch_size,
    std::string feature_rspecifier,
    std::string online_ivector_rspecifier,
    std::string ivector_rspecifier,
    std::string utt2spk_rspecifier,
    CudaFst &decode_fst_cuda,
    int32 number_threads,
    CudaLatticeDecoderConfig &config,
    const TransitionModel &trans_model,
    const fst::SymbolTable *word_syms,
    BaseFloat acoustic_scale,
    bool determinize,
    bool allow_partial,
    Int32VectorWriter *alignments_writer,
    Int32VectorWriter *words_writer,
    CompactLatticeWriter *compact_lattice_writer,
    LatticeWriter *lattice_writer,
    double *like_sum, // on success, adds likelihood to this.
    int64 *frame_sum, // on success, adds #frames to this.
    int32 *num_done, // on success (including partial decode), increments this.
    int32 *num_err,  // on failure, increments this.
    int32 *num_partial, // If partial decode(final-state not reached),increment.
    int32 num_max_chunks,
    int32 num_max_utts
    );  

  // The main function. 
  // (1) We create multi gpu decoding threads with class MultiThreader.
  // (2) We create a single thread to conduct batch computing.
  // (3) In this function, we read in the features and ivectors in loop. For
  //     here, the WaitingUtterancesRepository is used to coordinate each part.
  void Run();

 private:
  // Some variables for BatchComputer. For details, check the reference of
  // class BatchComputer.
  NnetSimpleComputationOptions opts_;
  // The neural network that we're going to do the computation with.
  const Nnet &nnet_;
  // Vector of priors. We subtract the log of these priors from the nnet output
  // to generate log-likelihoods.
  const VectorBase<BaseFloat> &priors_;
  int32 online_ivector_period_;
  // This option is used to control whether we deal with "shorter-than-chunk
  // -size" utterances specially.
  bool ensure_exact_final_context_;
  // The capacity of the minibatch.
  int32 minibatch_size_;
  const std::string feature_rspecifier_;
  const std::string online_ivector_rspecifier_;
  const std::string ivector_rspecifier_;
  const std::string utt2spk_rspecifier_;
  BatchComputer* batch_computer_; // ownership of the pointer
  
  // Some variables for Decoder. It will be used to initialize
  // class DecodeUtteranceLatticeClassCuda, in which LatticeFasterDecoderCuda
  // is used to decode.
  const CudaFst &decode_fst_cuda_;
  int32 num_threads_;  // The number of decoding threads
  CudaLatticeDecoderConfig &config_;
  const TransitionModel &trans_model_;
  const fst::SymbolTable *word_syms_;
  BaseFloat acoustic_scale_;
  bool determinize_;
  bool allow_partial_;
  Int32VectorWriter *alignments_writer_;
  Int32VectorWriter *words_writer_;
  CompactLatticeWriter *compact_lattice_writer_;
  LatticeWriter *lattice_writer_;
  double *like_sum_;
  int64 *frame_sum_;
  int32 *num_done_;
  int32 *num_err_;
  int32 *num_partial_;

  // Some variables which are used to connect BatchComputer and Decoder
  // This part can be understand as a public territory.
  std::mutex *utt_mutex_;
  WaitingUtterancesRepository *repository_;

  int32 num_max_chunks_; // The maximum number of chunks in GPU memory
  int32 num_max_utts_;  // The maximum number of utterances is processing in
                        // BatchComputer
  int32 chunk_counter_ = 0; // It is used to remember the number of chunks is
                            // available now

  // The key is utt_id. The value is a queue which contains all the pointers
  // to each posterior chunk. In BatchComputer, we will new the "CuMatrix"
  // space to store posterior chunk.
  unordered_map<std::string,
    std::queue<const CuMatrix<BaseFloat>* > > finished_inf_utts_;

  // record the number of chunk is processed for each utterance. It will be 
  // pass to BatchComputer to show what point the decoders are at in
  // each utterance.
  unordered_map<std::string, size_t> finished_dec_utts_;
  unordered_map<std::string, bool> is_end_;
  unordered_map<std::string, Semaphore> utts_semaphores_; 
};

} // end namespace nnet3
} // end namespace kaldi

#endif
